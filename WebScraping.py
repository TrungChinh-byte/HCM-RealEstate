# -*- coding: utf-8 -*-
"""Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eonAug6-8WO6H26uu7KXwtRnboQp9pCO
"""

!pip install pymongo

import requests # requests, BeautifulSoup for web-scraping
from bs4 import BeautifulSoup
import re
import time
import concurrent.futures # multithreading
import json
from pymongo import MongoClient, errors #import mongodb
from pymongo.server_api import ServerApi

# Header for browser
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'
}

# get real_estate links from 1 main_page
def scrape_listing_links_bs4(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.select('a.card-cm')
        listing_urls = [link['href'] for link in links if link.has_attr('href')]
        return listing_urls
    except Exception as e:
        print(f"Error getting links from {url}: {e}")
        return []

# get real_estate links from multiple main_page
def scrape_multiple_pages(base_url, page_start, page_end):
    urls = [f"{base_url}/p{page}" for page in range(page_start, page_end + 1)]
    all_links = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(scrape_listing_links_bs4, urls)
        for page_links in results:
            all_links.extend(page_links)
    return all_links

# Crawling info from each real_estate page
def scrape_detail_info_bs4(url):
    data = {'link': url}

    #time.sleep(1)

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        ## Upload_Date and ID of post
        values = soup.select('.col-md-12 .flex-grow-1 .col .value')
        values = [v.text.strip() for v in values if v.text.strip()]
        if values:
          data['date'] = values[0]
          data['id'] = values[-1]
        else:
          data['date'] = None
          data['id'] = None

        ## Get owner name
        name_elem = soup.select_one('.profile-avatar span.name')
        data['name'] = name_elem.text.strip() if name_elem else None

        ##  Get Phone Number
        phone_div = soup.find('div', attrs={'data-show': True})
        if phone_div:
            data['phone'] = phone_div['data-show'].split()[0]
        else:
            data['phone'] = None

        ## City and District
        breadcrumbs = soup.select('.re__breadcrumb a.re__link-se')
        texts = [b.text.strip() for b in breadcrumbs if b.text.strip()]
        if len(texts) >= 2:
          data['city'] = texts[-2]
          data['district'] = texts[-1]
        else:
          data['city'] = None
          data['district'] = None

        ## Address
        address_elem = soup.select_one('.slide-description .footer')
        if address_elem:
          data['address'] = address_elem.text.strip().split('\n')[0]
        else:
          data['address'] = None

        ## Title
        content_elements = soup.select('.content')
        if content_elements:
            data['title'] = content_elements[0].text.strip()
        else:
            data['title'] = None

        ## Information : Area, Bedrooms, Toilets, Price, Authorization, ...
        label_elements = soup.select('.row-cols-sm-2 .line-label')
        info_elements = soup.select('.row-cols-sm-2 .line-text')

        for label, info in zip(label_elements, info_elements):
              data[label.text.strip()] = info.text.strip()

        ## Description
        desc_elem = soup.find(id='more1')
        if desc_elem:
          data['Description'] = desc_elem.get_text(strip=True)
        else :
          data['Description'] = None

    except Exception as e:
        print(f"Error scraping {url}: {e}")

    return data

# Insert data into MongoDB Atlas
def insert_mongo_db(jsonarray, client: MongoClient, db_name: str, collection_name: str):
    db = client[db_name]
    collection = db[collection_name]
    collection.create_index([("id", 1)], unique=True)

    try:
        collection.insert_many(jsonarray, ordered=False)
        print("Insert Complete !!!")
    except errors.BulkWriteError as e:
        print("Duplicated Id json !!!", e.details)

    count = collection.count_documents({})
    print("Count of documents:", count)

#Main
if __name__ == "__main__":
    # Scraping Links
    base_url = 'https://batdongsan.vn/ban-nha-ho-chi-minh'
    start_page = 1
    end_page = 1000

    for i in range(start_page, end_page + 1, 10):
        sp = i
        ep = min(i + 9, end_page)

        # Scraping Links
        print(f"Getting links from pages {sp} to {ep}")
        listings = scrape_multiple_pages(base_url, sp, ep)
        print(f"Done getting {len(listings)} links.")

        # Crawling info
        all_data = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = executor.map(scrape_detail_info_bs4, listings)
            for result in results:
                all_data.append(result)
                print(f"- Post Title: {result.get('title')}")

        # saving .json files
        json_file_name = f"real_estate_data_{sp}_{ep}.json"
        with open(json_file_name, "w", encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"Saved json_file : {json_file_name}")

        # Adding to MongoDB Atlas
        uri = "mongodb+srv://ChinhChumChim123:123@cluster0.vqoitmj.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
        client = MongoClient(uri, server_api=ServerApi('1')) # Create a new client and connect to the server
        db_name = 'scraping'
        collection_name = 'real_estate'

        try:
            client.admin.command('ping')
            print("Successfully connected to MongoDB Atlas.")
        except Exception as e:
            print("MongoDB Connection Failed:", e)

        insert_mongo_db(all_data, client, db_name, collection_name)

